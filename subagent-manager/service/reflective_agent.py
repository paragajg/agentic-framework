"""
Reflective Agent with PLAN -> EXECUTE -> VALIDATE -> REFINE loop.

Module: subagent-manager/service/reflective_agent.py

Provides agentic behavior with internal reasoning, planning, validation,
and iterative refinement capabilities.
"""

import logging
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from pydantic import BaseModel, Field


logger = logging.getLogger(__name__)


class ReflectionPhase(str, Enum):
    """Current phase in the reflective loop."""

    PLANNING = "planning"
    EXECUTING = "executing"
    VALIDATING = "validating"
    REFINING = "refining"
    COMPLETED = "completed"
    FAILED = "failed"


class PlanStep(BaseModel):
    """A single step in an execution plan."""

    step_id: str = Field(default_factory=lambda: str(uuid4())[:8])
    action: str = Field(..., description="Action to take")
    tool: Optional[str] = Field(None, description="Tool/skill to use")
    inputs: Dict[str, Any] = Field(default_factory=dict)
    expected_output: str = Field(..., description="What success looks like")
    fallback: Optional[str] = Field(None, description="Fallback if this fails")


class ExecutionPlan(BaseModel):
    """Complete execution plan generated by agent."""

    plan_id: str = Field(default_factory=lambda: str(uuid4()))
    task_understanding: str = Field(..., description="Agent's understanding of the task")
    approach: str = Field(..., description="High-level approach")
    steps: List[PlanStep] = Field(default_factory=list)
    success_criteria: List[str] = Field(default_factory=list)
    potential_risks: List[str] = Field(default_factory=list)
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    created_at: datetime = Field(default_factory=datetime.utcnow)


class ValidationResult(BaseModel):
    """Result of validating execution output."""

    is_valid: bool = Field(..., description="Whether output meets criteria")
    score: float = Field(0.0, ge=0.0, le=1.0, description="Quality score")
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    suggestions: List[str] = Field(default_factory=list)
    criteria_met: Dict[str, bool] = Field(default_factory=dict)


class RefinementAction(BaseModel):
    """Action to take for refinement."""

    action_type: str = Field(..., description="Type: retry, modify, escalate, abort")
    reasoning: str = Field(..., description="Why this action was chosen")
    modifications: Dict[str, Any] = Field(default_factory=dict)
    new_approach: Optional[str] = Field(None)


@dataclass
class ReflectionState:
    """State of a reflective agent execution."""

    task_id: str
    original_task: str
    current_phase: ReflectionPhase = ReflectionPhase.PLANNING
    iteration: int = 0
    max_iterations: int = 3

    # Planning
    current_plan: Optional[ExecutionPlan] = None
    plan_history: List[ExecutionPlan] = field(default_factory=list)

    # Execution
    execution_results: List[Dict[str, Any]] = field(default_factory=list)
    current_step_index: int = 0

    # Validation
    validation_results: List[ValidationResult] = field(default_factory=list)

    # Refinement
    refinement_actions: List[RefinementAction] = field(default_factory=list)

    # Outcome
    final_output: Optional[Dict[str, Any]] = None
    success: bool = False
    error_message: Optional[str] = None

    # Metrics
    started_at: datetime = field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None
    total_llm_calls: int = 0
    total_tool_calls: int = 0


class ReflectiveAgentConfig(BaseModel):
    """Configuration for reflective agent behavior."""

    # Planning
    enable_planning: bool = Field(True, description="Enable planning phase")
    require_explicit_plan: bool = Field(False, description="Always generate formal plan")
    min_confidence_to_execute: float = Field(0.6, description="Min confidence to proceed")

    # Execution
    max_iterations: int = Field(3, description="Max refinement iterations")
    step_timeout_seconds: int = Field(30, description="Timeout per step")

    # Validation
    enable_self_validation: bool = Field(True, description="Agent validates own output")
    validation_strictness: str = Field("medium", description="low/medium/high")
    min_quality_score: float = Field(0.7, description="Min score to accept")

    # Refinement
    enable_refinement: bool = Field(True, description="Enable refinement loop")
    refinement_strategy: str = Field("adaptive", description="adaptive/conservative/aggressive")
    max_retries_per_step: int = Field(2, description="Max retries per step")

    # Reflection
    enable_reflection: bool = Field(True, description="Enable post-execution reflection")
    save_reflection_history: bool = Field(True, description="Save for learning")


class ReflectiveAgent:
    """
    Agent with internal PLAN -> EXECUTE -> VALIDATE -> REFINE loop.

    This implements true agentic behavior where the agent:
    1. PLANS: Analyzes the task and creates an execution plan
    2. EXECUTES: Runs the plan step by step
    3. VALIDATES: Checks if output meets success criteria
    4. REFINES: If validation fails, reasons about errors and adjusts approach

    Example:
        agent = ReflectiveAgent(llm_adapter, config)
        result = await agent.execute_task(
            "Find all Python files with security vulnerabilities"
        )
    """

    def __init__(
        self,
        llm_adapter: Any,
        config: Optional[ReflectiveAgentConfig] = None,
        tool_executor: Optional[Any] = None,
        validator: Optional[Any] = None,
    ):
        """
        Initialize reflective agent.

        Args:
            llm_adapter: LLM adapter for generating responses
            config: Agent configuration
            tool_executor: Executor for tools/skills
            validator: Schema validator for outputs
        """
        self.llm = llm_adapter
        self.config = config or ReflectiveAgentConfig()
        self.tool_executor = tool_executor
        self.validator = validator

        # Import prompts
        from .planning_prompts import PlanningPrompts

        self.prompts = PlanningPrompts()

    async def execute_task(
        self,
        task: str,
        context: Optional[Dict[str, Any]] = None,
        constraints: Optional[List[str]] = None,
    ) -> ReflectionState:
        """
        Execute a task with full reflective loop.

        Args:
            task: Task description
            context: Additional context
            constraints: Constraints to follow

        Returns:
            ReflectionState with results
        """
        state = ReflectionState(
            task_id=str(uuid4()),
            original_task=task,
            max_iterations=self.config.max_iterations,
        )

        logger.info(f"Starting reflective execution for task: {task[:100]}...")

        try:
            while state.iteration < state.max_iterations:
                state.iteration += 1
                logger.info(f"Iteration {state.iteration}/{state.max_iterations}")

                # PHASE 1: PLAN
                if self.config.enable_planning:
                    state.current_phase = ReflectionPhase.PLANNING
                    plan = await self._plan(task, context, constraints, state)
                    state.current_plan = plan
                    state.plan_history.append(plan)

                    if plan.confidence < self.config.min_confidence_to_execute:
                        logger.warning(
                            f"Plan confidence {plan.confidence} below threshold "
                            f"{self.config.min_confidence_to_execute}"
                        )
                        # Try to refine plan before executing
                        continue

                # PHASE 2: EXECUTE
                state.current_phase = ReflectionPhase.EXECUTING
                execution_result = await self._execute(state)
                state.execution_results.append(execution_result)

                # PHASE 3: VALIDATE
                if self.config.enable_self_validation:
                    state.current_phase = ReflectionPhase.VALIDATING
                    validation = await self._validate(execution_result, state)
                    state.validation_results.append(validation)

                    if validation.is_valid and validation.score >= self.config.min_quality_score:
                        # Success!
                        state.current_phase = ReflectionPhase.COMPLETED
                        state.success = True
                        state.final_output = execution_result
                        break
                else:
                    # No validation, assume success
                    state.current_phase = ReflectionPhase.COMPLETED
                    state.success = True
                    state.final_output = execution_result
                    break

                # PHASE 4: REFINE (if validation failed)
                if self.config.enable_refinement:
                    state.current_phase = ReflectionPhase.REFINING
                    refinement = await self._refine(state)
                    state.refinement_actions.append(refinement)

                    if refinement.action_type == "abort":
                        state.current_phase = ReflectionPhase.FAILED
                        state.error_message = refinement.reasoning
                        break
                else:
                    # No refinement, fail
                    state.current_phase = ReflectionPhase.FAILED
                    state.error_message = "Validation failed and refinement disabled"
                    break

            # Check if we exhausted iterations
            if state.iteration >= state.max_iterations and not state.success:
                state.current_phase = ReflectionPhase.FAILED
                state.error_message = f"Max iterations ({state.max_iterations}) reached"

        except Exception as e:
            logger.exception(f"Reflective execution failed: {e}")
            state.current_phase = ReflectionPhase.FAILED
            state.error_message = str(e)

        finally:
            state.completed_at = datetime.utcnow()

            # Optional reflection for learning
            if self.config.enable_reflection and state.success:
                await self._reflect(state)

        return state

    async def _plan(
        self,
        task: str,
        context: Optional[Dict[str, Any]],
        constraints: Optional[List[str]],
        state: ReflectionState,
    ) -> ExecutionPlan:
        """
        Generate execution plan for the task.

        Args:
            task: Task description
            context: Additional context
            constraints: Constraints to follow
            state: Current reflection state

        Returns:
            ExecutionPlan with steps and success criteria
        """
        logger.info("PLAN phase: Generating execution plan...")

        # Build planning prompt
        prompt = self.prompts.build_planning_prompt(
            task=task,
            context=context,
            constraints=constraints,
            previous_attempts=state.plan_history,
            available_tools=self._get_available_tools(),
        )

        # Call LLM for planning
        response = await self._call_llm(prompt, state)
        state.total_llm_calls += 1

        # Parse plan from response
        plan = self._parse_plan(response)

        logger.info(
            f"Plan generated: {len(plan.steps)} steps, "
            f"confidence: {plan.confidence:.2f}"
        )

        return plan

    async def _execute(self, state: ReflectionState) -> Dict[str, Any]:
        """
        Execute the current plan.

        Args:
            state: Current reflection state

        Returns:
            Execution results
        """
        logger.info("EXECUTE phase: Running plan steps...")

        results = {
            "steps_completed": [],
            "steps_failed": [],
            "outputs": {},
            "errors": [],
        }

        plan = state.current_plan
        if not plan:
            # No plan, execute directly
            return await self._execute_direct(state.original_task, state)

        for i, step in enumerate(plan.steps):
            state.current_step_index = i
            logger.info(f"Executing step {i + 1}/{len(plan.steps)}: {step.action}")

            try:
                step_result = await self._execute_step(step, state)
                results["steps_completed"].append(step.step_id)
                results["outputs"][step.step_id] = step_result
                state.total_tool_calls += 1

            except Exception as e:
                logger.warning(f"Step {step.step_id} failed: {e}")
                results["steps_failed"].append(step.step_id)
                results["errors"].append(str(e))

                # Try fallback if available
                if step.fallback:
                    logger.info(f"Trying fallback: {step.fallback}")
                    # Could implement fallback logic here

        results["success_rate"] = (
            len(results["steps_completed"]) / len(plan.steps) if plan.steps else 0
        )

        return results

    async def _validate(
        self, execution_result: Dict[str, Any], state: ReflectionState
    ) -> ValidationResult:
        """
        Validate execution results against success criteria.

        Args:
            execution_result: Results from execution phase
            state: Current reflection state

        Returns:
            ValidationResult with score and feedback
        """
        logger.info("VALIDATE phase: Checking results...")

        # Build validation prompt
        prompt = self.prompts.build_validation_prompt(
            task=state.original_task,
            plan=state.current_plan,
            execution_result=execution_result,
            success_criteria=(
                state.current_plan.success_criteria if state.current_plan else []
            ),
            strictness=self.config.validation_strictness,
        )

        # Call LLM for validation
        response = await self._call_llm(prompt, state)
        state.total_llm_calls += 1

        # Parse validation result
        validation = self._parse_validation(response, state.current_plan)

        logger.info(
            f"Validation: valid={validation.is_valid}, "
            f"score={validation.score:.2f}, "
            f"errors={len(validation.errors)}"
        )

        return validation

    async def _refine(self, state: ReflectionState) -> RefinementAction:
        """
        Analyze failures and determine refinement action.

        Args:
            state: Current reflection state

        Returns:
            RefinementAction describing next steps
        """
        logger.info("REFINE phase: Analyzing failures...")

        last_validation = state.validation_results[-1] if state.validation_results else None
        last_execution = state.execution_results[-1] if state.execution_results else None

        # Build refinement prompt
        prompt = self.prompts.build_refinement_prompt(
            task=state.original_task,
            plan=state.current_plan,
            execution_result=last_execution,
            validation_result=last_validation,
            iteration=state.iteration,
            max_iterations=state.max_iterations,
            previous_refinements=state.refinement_actions,
            strategy=self.config.refinement_strategy,
        )

        # Call LLM for refinement reasoning
        response = await self._call_llm(prompt, state)
        state.total_llm_calls += 1

        # Parse refinement action
        refinement = self._parse_refinement(response)

        logger.info(
            f"Refinement action: {refinement.action_type}, "
            f"reasoning: {refinement.reasoning[:100]}..."
        )

        # Apply refinement to state
        if refinement.action_type == "modify" and refinement.new_approach:
            # Create modified plan for next iteration
            state.current_plan = await self._create_modified_plan(
                state.current_plan, refinement, state
            )

        return refinement

    async def _reflect(self, state: ReflectionState) -> None:
        """
        Post-execution reflection for learning.

        Args:
            state: Completed reflection state
        """
        logger.info("REFLECT phase: Analyzing execution for learning...")

        # Could implement learning/memory here
        # - What worked well
        # - What could be improved
        # - Patterns to remember

        reflection_summary = {
            "task": state.original_task,
            "iterations_used": state.iteration,
            "total_llm_calls": state.total_llm_calls,
            "total_tool_calls": state.total_tool_calls,
            "success": state.success,
            "final_score": (
                state.validation_results[-1].score
                if state.validation_results
                else None
            ),
        }

        logger.info(f"Reflection summary: {reflection_summary}")

    # Helper methods

    async def _call_llm(self, prompt: str, state: ReflectionState) -> str:
        """Call LLM with prompt."""
        # This should use the actual LLM adapter
        if hasattr(self.llm, "generate"):
            return await self.llm.generate(prompt)
        elif hasattr(self.llm, "chat"):
            return await self.llm.chat(prompt)
        else:
            raise ValueError("LLM adapter must have generate() or chat() method")

    async def _execute_step(
        self, step: PlanStep, state: ReflectionState
    ) -> Dict[str, Any]:
        """Execute a single plan step."""
        if step.tool and self.tool_executor:
            return self.tool_executor.execute(step.tool, step.inputs)
        else:
            # No tool specified, use LLM to execute
            prompt = f"Execute this action: {step.action}\nInputs: {step.inputs}"
            response = await self._call_llm(prompt, state)
            return {"response": response}

    async def _execute_direct(
        self, task: str, state: ReflectionState
    ) -> Dict[str, Any]:
        """Execute task directly without plan."""
        response = await self._call_llm(task, state)
        return {"response": response}

    async def _create_modified_plan(
        self,
        original_plan: Optional[ExecutionPlan],
        refinement: RefinementAction,
        state: ReflectionState,
    ) -> ExecutionPlan:
        """Create modified plan based on refinement."""
        prompt = self.prompts.build_plan_modification_prompt(
            original_plan=original_plan,
            refinement=refinement,
            available_tools=self._get_available_tools(),
        )
        response = await self._call_llm(prompt, state)
        return self._parse_plan(response)

    def _get_available_tools(self) -> List[str]:
        """Get list of available tools."""
        if self.tool_executor and hasattr(self.tool_executor, "list_tools"):
            return self.tool_executor.list_tools()
        return []

    def _parse_plan(self, response: str) -> ExecutionPlan:
        """Parse LLM response into ExecutionPlan."""
        # Simple parsing - in production, use structured output
        import json
        import re

        # Try to find JSON in response
        json_match = re.search(r"\{[\s\S]*\}", response)
        if json_match:
            try:
                data = json.loads(json_match.group())
                return ExecutionPlan(**data)
            except (json.JSONDecodeError, ValueError):
                pass

        # Fallback: create simple plan from response
        return ExecutionPlan(
            task_understanding=response[:200],
            approach=response[:500],
            steps=[
                PlanStep(
                    action="Execute task based on understanding",
                    expected_output="Task completed successfully",
                )
            ],
            success_criteria=["Task objective achieved"],
            confidence=0.7,
        )

    def _parse_validation(
        self, response: str, plan: Optional[ExecutionPlan]
    ) -> ValidationResult:
        """Parse LLM response into ValidationResult."""
        import json
        import re

        # Try to find JSON in response
        json_match = re.search(r"\{[\s\S]*\}", response)
        if json_match:
            try:
                data = json.loads(json_match.group())
                return ValidationResult(**data)
            except (json.JSONDecodeError, ValueError):
                pass

        # Fallback: infer from response text
        response_lower = response.lower()
        is_valid = "success" in response_lower or "valid" in response_lower
        has_errors = "error" in response_lower or "fail" in response_lower

        return ValidationResult(
            is_valid=is_valid and not has_errors,
            score=0.8 if is_valid else 0.3,
            errors=["Validation inconclusive"] if not is_valid else [],
        )

    def _parse_refinement(self, response: str) -> RefinementAction:
        """Parse LLM response into RefinementAction."""
        import json
        import re

        # Try to find JSON in response
        json_match = re.search(r"\{[\s\S]*\}", response)
        if json_match:
            try:
                data = json.loads(json_match.group())
                return RefinementAction(**data)
            except (json.JSONDecodeError, ValueError):
                pass

        # Fallback: determine action from response
        response_lower = response.lower()

        if "abort" in response_lower or "give up" in response_lower:
            action_type = "abort"
        elif "retry" in response_lower:
            action_type = "retry"
        elif "modify" in response_lower or "change" in response_lower:
            action_type = "modify"
        else:
            action_type = "retry"

        return RefinementAction(
            action_type=action_type,
            reasoning=response[:500],
        )
